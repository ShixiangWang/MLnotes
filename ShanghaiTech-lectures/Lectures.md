# ShanghaiTech CS286

超参数控制：

- 模型复杂度
- 模型复杂度类型
- 优化算法
- 模型类型

## 传统机器学习


### 回归

#### 线性回归

$$
\hat{y} = h_{\theta}(x) = \theta_0 + \theta_1x_1 + ... +  \theta_nx_n
$$

向量表示为参数向量 $\theta$ 与$X$ 的点积。

用均方误差（root mean square error, RMSE）表示拟合优度。

实践中直接使用 MSE（mean square error）构建损失函数。
$$
J(\theta) = MSE
$$

#### 梯度下降（Gradient descent）

- 随机化参数
- 使用梯度下降改变参数（偏微分），优化损失函数



问题：

- 局部最优
- 平台停滞



其他类型：

- 批量梯度下降
- 随机梯度下降（SGD）
- 小批量梯度下降



#### 多项式回归

对变量加幂次建模。



#### 学习曲线

随着训练集增大，训练和验证集损失变化。



#### 正则化线性模型

引入惩罚项。

- 岭回归：让参数变小
- Lasso 回归：让参数变得稀疏（参数减少）



#### 逻辑回归

$$
\hat{p} = h_{\theta}(x)\\

\hat{y} = 1 \space if \space \hat{p} > 0.5
$$



#### Softmax 回归

用于多分类。



### 贝叶斯统计

#### 条件概率

- 联合概率
- 条件概率
- 独立事件

#### 贝叶斯定理

贝叶斯公式

![image-20200915192833364](https://gitee.com/ShixiangWang/ImageCollection/raw/master/png/20200915192833.png)

主观判断和客观事件以概率进行联系。

最大似然法（ML）
$$
P(D|\alpha, M)
$$
给定模型 $M$ 和参数 $\alpha$，给定数据 D 的似然度。

由此反向找到对应的参数。

优点：

- 一致性好
- 高效
- 参数转换不变异

缺点：

- 数据非常少时结果差

最大化后验概率（MAP）
$$
P(\alpha|D, M)
$$
参数先验概率 
$$
P(\alpha|M)
$$


MCMC（马尔可夫链蒙特卡洛方法） 推断后验概率使得贝叶斯计算变得可行。

### 支持向量机

